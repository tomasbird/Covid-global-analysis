---
title: "PAN-E global analysis"
author: "Tom Bird"
date: "12/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PAN E global analysis
This brings together our various analyses into a working document

R document for PAN E global analysis

```{r data}
# google mobility data from Cerren
googdat=readRDS("Data/google.rds")

# air traffic data
airdat=read.csv("Data/RawAirData.csv")

# NO2 data

# AIS shipping data (David)
```

## Google mobility
We are still working out what the storyline is for the google mobility data and how they support the empirical, anecdotal and twitter analyses 

```{r google Analysis}
# Read in google data
google <- read.csv("Global_Mobility_Report.csv")

# R reads Namibia's iso2 code is as an "is.na" object, so we will rename it to NA
google$country_region_code[is.na(google$country_region_code)] <-"NA"

# Add the country codes so we ensure everything matches 
library(countrycode)
google$country_alpha2 <- google$country_region_code
google$country_alpha3 <- countrycode(google$country_region_code, origin = 'iso2c', destination = 'iso3c')
google$country_name <- countrycode(google$country_region_code, origin = 'iso2c', destination = 'country.name')

# Add Julian date
google$julian<- (strptime(google$date, "%Y-%m-%d")$yday) + 1 # need to add 1 day because 2020 is a leap year

library(dplyr)
# Select the columns that will be used in further analyses
google <- google %>% select(country_name, country_alpha2, country_alpha3,
                            sub_region_1, sub_region_2, iso_3166_2_code,
                            date, julian, residential = residential_percent_change_from_baseline, 
                            parks = parks_percent_change_from_baseline)


# Visualise a subset of the data
# A weekend effect may need to be included in the model
library(ggplot2)
ggplot(filter(google, country_alpha3 =="GBR" ), aes(x=julian, y=residential)) + 
  geom_point() + 
  geom_vline(xintercept = 83, colour = "red")


# Add week days 
google$weekend<- weekdays(as.Date(google$date), abbr = TRUE)

# Assign yes/no to weekend and week day
google$weekend[google$weekend %in% c("Sat", "Sun") ] <- c("YES", "YES")
google$weekend[google$weekend %in% c("Mon", "Tue", "Wed", "Thu", "Fri") ] <- c("NO", "NO", "NO", "NO", "NO")


# We will run different models on the data with and without "sub_region_1"
# We will subset these two types
subregion <- google %>% group_by(country_alpha2) %>% summarise(subregions = n_distinct(sub_region_1))

# join the number of subregions
google <- left_join(google, subregion)


# Load in confinement data from Oxford Covid-19 Government Response Tracker (OxCGRT)
# Data info: https://github.com/OxCGRT/covid-policy-tracker

library(RCurl)
confinement <- getURL("https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest_withnotes.csv")
confinement <- read.csv(text = confinement)

confinement$julian<- (strptime(confinement$Date, "%Y%m%d")$yday) + 1 # need to add 1 day because 2020 is a leap year

confinement <- confinement %>% select(CountryName, country_alpha3 = CountryCode, Date, julian, StringencyIndex, 
                      C1_School.closing, C2_Workplace.closing, C3_Cancel.public.events,
                      C4_Restrictions.on.gatherings, C5_Close.public.transport,
                      C6_Stay.at.home.requirements, C7_Restrictions.on.internal.movement, 
                      C8_International.travel.controls)

# Join the google and the OxCGRT data
google <- left_join(google, confinement)
```

## Subsetting countries with and without subregions

We will run different models on the data with and without "sub_region_1".
Here we subset the `google` dataframe into `google_no_sub` and `google_sub`.

```{r}

# Identify countries with subregions
subregion <- google %>% group_by(country_alpha2) %>% summarise(subregions = n_distinct(sub_region_1))

# join the number of subregions to the google dataframe
google <- left_join(google, subregion)

# Filter countries without subregions
google_no_sub <- filter(google, subregions == 1)

# Filter countries with subregions
google_sub <- filter(google, subregions > 1)

# The countries with subregions have an overall trend. We will delete this here. 
library(naniar)
google_sub<- google_sub %>% replace_with_na(replace = list(sub_region_1 = ""))
google_sub<- google_sub[!is.na(google_sub$sub_region_1), ]

```


## TEST: Modelling countries WITHOUT subregions

This only uses a random lockdown date so that we can start playing with the models

```{r}

# FOR TEST ONLY: ASSIGN A RANDOM LOCKDOWN DATE FOR ALL COUNTRIES
google_no_sub$julian_lockdown <- 71


# Assign before and after lockdown
google_no_sub <- mutate(google_no_sub, lockdown = ifelse(julian >= julian_lockdown, "After", "Before"))
google_no_sub$lockdown<- factor(google_no_sub$lockdown, levels =c("Before","After"))

# Make a list of all the countries with no subregions
# This will be called on in the loop below and the tvalues will be placed here
no_sub_countries <- google_no_sub %>% distinct(country_alpha3, country_name)


library(mgcv)

# RESIDENTIAL MODEL

#Assign NA to all countries
no_sub_countries$tval_residential <- NA

# Run a loop that extracts the t-value from the gam model run for each country
for (i in 1:nrow(no_sub_countries)) { 
  tryCatch({

# Run GAM model for each country in the no_sub_countries dataframe
gam1 <- gam(data=filter(google_no_sub, country_alpha3 == no_sub_countries$country_alpha3[i]), residential ~ lockdown + weekend + s(julian)) 

# Extract the t-value and paste it into the no_sub_countries dataframe
gam.table <- summary(gam1)
gam.table <- gam.table$p.table
no_sub_countries$tval_residential[i] <- gam.table[2,3]

  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}



# PARKS MODEL

#Assign NA to all countries
no_sub_countries$tval_parks <- NA

# Run a loop that extracts the t-value from the gam model run for each country
for (i in 1:nrow(no_sub_countries)) { 
    tryCatch({

# Run GAM model for each country in the no_sub_countries dataframe
gam2 <- gam(data=filter(google_no_sub, country_alpha3 == no_sub_countries$country_alpha3[i]), parks ~ lockdown + weekend + s(julian)) 

# Extract the t-value and paste it into the no_sub_countries dataframe
gam.table <- summary(gam2)
gam.table <- gam.table$p.table
no_sub_countries$tval_parks[i] <- gam.table[2,3]

  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}

```





```{r google figures}

```


This section for google mobility:

## Air traffic
```{r Air traffic data, echo=FALSE}
library(reshape2)
# manipulate Air data
airdat.long=melt(airdat, id.vars=c("Year", "Data", "Countries"))
airdat.long$variable=gsub("\\.", "/", airdat.long$variable)
airdat.long$date=with(airdat.long, as.Date(paste(variable, Year, sep="/"), format="%b/%d/%Y" ))

# merge with lockdown
```


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
