---
title: "PAN-E global analysis"
author: "Tom Bird & Cerren Richards"
date: "12/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PAN E global analysis
This brings together our various analyses into a working document

R document for PAN E global analysis

```{r data}
# google mobility data from Cerren
googdat=readRDS("Data/google.rds")

# air traffic data
airdat=read.csv("Data/RawAirData.csv")

# NO2 data

# AIS shipping data (David)
```

## Google mobility
We are still working out what the storyline is for the google mobility data and how they support the empirical, anecdotal and twitter analyses 

**Data Description**: How visits and length of stay at different places change compared to a baseline.
    - Parks: national parks, public beaches, marinas, dog parks, plazas, and public gardens.
    - Residential: places of residence
    
**Baseline**: Median value, for the corresponding day of the week, during the 5- week period Jan 3â€“Feb 6, 2020

**Download data and methods**:
* https://www.google.com/covid19/mobility/
* https://support.google.com/covid19-mobility/answer/9824897?hl=en&ref_topic=9822927


### Preparing the dataframe for further analyses
In this R chunk, we join the Google Mobility Data with the Oxford Covid-19 Government Response Tracker data. We ensure all country codes are matching, add a Julian Date, and a weekend column.

<<<<<<< HEAD
```{r google Analysis}
# Read in google data
google <- read.csv("Global_Mobility_Report.csv")
>>>>>>> 392729d2bb8a58c27a9ab05e870245fb3ec08252

# R reads Namibia's iso2 code is as an "is.na" object, so we will rename it to NA
google$country_region_code[is.na(google$country_region_code)] <-"NA"

# Add the country codes so we ensure everything matches 
library(countrycode)
google$country_alpha2 <- google$country_region_code
google$country_alpha3 <- countrycode(google$country_region_code, origin = 'iso2c', destination = 'iso3c')
google$country_name <- countrycode(google$country_region_code, origin = 'iso2c', destination = 'country.name')

# Add Julian date
google$julian<- (strptime(google$date, "%Y-%m-%d")$yday) + 1 # need to add 1 day because 2020 is a leap year

library(dplyr)
# Select the columns that will be used in further analyses
google <- google %>% select(country_name, country_alpha2, country_alpha3,
                            sub_region_1, sub_region_2, iso_3166_2_code,
                            date, julian, residential = residential_percent_change_from_baseline, 
                            parks = parks_percent_change_from_baseline)


# Visualise a subset of the data
# A weekend effect may need to be included in the model
library(ggplot2)
ggplot(filter(google, country_alpha3 =="GBR" ), aes(x=julian, y=residential)) + 
  geom_point() + 
  geom_vline(xintercept = 83, colour = "red")


# Add week days 
google$weekend<- weekdays(as.Date(google$date), abbr = TRUE)

# Assign yes/no to weekend and week day
google$weekend[google$weekend %in% c("Sat", "Sun") ] <- c("YES", "YES")
google$weekend[google$weekend %in% c("Mon", "Tue", "Wed", "Thu", "Fri") ] <- c("NO", "NO", "NO", "NO", "NO")

# Load in confinement data from Oxford Covid-19 Government Response Tracker (OxCGRT)
# Data info: https://github.com/OxCGRT/covid-policy-tracker

library(RCurl)
confinement <- getURL("https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest_withnotes.csv")
confinement <- read.csv(text = confinement)

confinement$julian<- (strptime(confinement$Date, "%Y%m%d")$yday) + 1 # need to add 1 day because 2020 is a leap year

confinement <- confinement %>% select(CountryName, country_alpha3 = CountryCode, Date, julian, StringencyIndex, 
                      C1_School.closing, C2_Workplace.closing, C3_Cancel.public.events,
                      C4_Restrictions.on.gatherings, C5_Close.public.transport,
                      C6_Stay.at.home.requirements, C7_Restrictions.on.internal.movement, 
                      C8_International.travel.controls)

# Join the google and the OxCGRT data
google <- left_join(google, confinement)
```


## Subsetting countries with and without subregions

We will run different models on the data with and without "sub_region_1".
Here we subset the `google` dataframe into `google_no_sub` and `google_sub`.

```{r}

# Identify countries with subregions
subregion <- google %>% group_by(country_alpha2) %>% summarise(subregions = n_distinct(sub_region_1))

# join the number of subregions to the google dataframe
google <- left_join(google, subregion)

# Filter countries without subregions
google_no_sub <- filter(google, subregions == 1)

# Filter countries with subregions
google_sub <- filter(google, subregions > 1)

# The countries with subregions have an overall trend. We will delete this here. 
library(naniar)
google_sub<- google_sub %>% replace_with_na(replace = list(sub_region_1 = ""))
google_sub<- google_sub[!is.na(google_sub$sub_region_1), ]

```



```{r google figures}

```


This section for google mobility:



Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
